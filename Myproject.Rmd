---
title: "R Notebook"
output: html_notebook
---


```{r}
tourism.data=readxl::read_xlsx("Tourism_new.xlsx")
head(tourism.data)
```

```{r}
dim(tourism.data)
```


```{r}
tail(tourism.data)
```


```{r}
# Monthly Seasonality (frequency set to 12 for monthly data)
tourism <- ts(tourism.data[,2],start= c(1997, 3),end = c(2020,3),frequency=12)
tourism
```


```{r}
plot(tourism)
```
The graph clearly shows a seasonality and an increasing trend i.e. a multiplicative variance. So to remove the variance, to convert it into a additive series, we need to find the log returns of the series. Let us first plot the log returns of the series and see how it looks. 

```{r}
log_tourism <- log(tourism)
plot(log_tourism)
```

We see that the variance has greatly reduced now. And now it shows a clear upward linearly increasing trend and seasonality. To remove the trend we can now apply diff() operator on the given time series. After applying difference lets plot and find out how it looks. As it has got both a trend and seasonality, so it seems that we need to apply both a simple difference diff(), and a seasonal difference operator diff(,12) with seasonality 12 to remove the trend and seasonality. 

```{r}
detrend_log_tourism <- diff(log_tourism)
plot(detrend_log_tourism)
```

```{r}
deseasoned_detrend_log_tourism <- diff(detrend_log_tourism, 12)
plot(diff(diff(deseasoned_detrend_log_tourism,12)))
```

Now let us see if there is still any seasonality left. we shall check this using Dickey-Fuller Unit Root Test using package "urca".


```{r}
library(urca)
summary(ur.df(diff(deseasoned_detrend_log_tourism,12), type = "none", lags = 1))
```

```{r}
library(urca)
summary(ur.df(diff(deseasoned_detrend_log_tourism,12), type = "trend", lags = 12))
```

Here we previously we have detrended the data, so applied ADF test using type = "none" with lag 1. 
The p value for z.lag.1 is far below 0.001, which shows that we can reject the NULL Hypothesis in favour of the alternate hypothesis. Which means that Unit root is not present and the data is stationary. 

We can now proceed forward to find the ACF and the PACF in order to model the data. We shall be using a package astsa for this purpose. 

```{r}
library(astsa)
acf2(diff(deseasoned_detrend_log_tourism,12), max.lag = 120)
```

```{r}
model_1 <- sarima(xdata = log_tourism,p = 0,d = 1, q = 2,P = 1,D = 2,Q = 1,S = 12, no.constant = T)
model_1$ttable
```


```{r}
2020-01 - 1988-03	
```

We have here 26 years of data, we shall be using 22 years of data to built the model and the rest 3 years for the prediction. That being said we shall use data from 1987-03 to 2017-03 ( 23 years) for building the model and the data from the rest three years from prediction. 

*Note: * There is a stiff fall the graph value on 2020-03-01 which is due to the *covid pandemic*, so we wont be considering the predicted deviation at that time point. 

Training: *1988-Mar to 2017-Mar*

```{r}
# Delimit training range̥
tourism.train <- window(log_tourism, end= c(2017,03))
tourism.train

```

Here we are removing the march months value, as its general behaviour is affceted by COVID-19 and hence can affect the accuracy later. 

Testing: *April 2017-Jan 2020*

```{r}
# Delimit testing range
tourism.test <- window(log_tourism, start=c(2017,03), end = c(2020, 01))
tourism.test
```


```{r}
plot(log_tourism,main="Tourist Arrival 1988-2017 (Log transformed)",ylab="Tourist arrival",xlab="Years")
lines(tourism.train,col="blue")
lines(tourism.test,col="green")
legend("bottomright",col=c("blue","green"),lty=1,legend=c("Training","Testing"))

```

Now that the data is divided into training and testing data set, we shall now fit model to a training data and the forecast on a testing data. 

### Model 1: SARIMA (p,d,q)*(P,D,Q)[12] where p = 2, d = 1, q = 1, P = 1, D = 1, Q = 1

```{r}
model_1 <- sarima(log(tourism.train), 2,1,1,1,1,1,12)
```


```{r}
model_1$ttable
```

In the previous model the p value of the coefficients shows that the null hypothesis against sar1 cannot be rejected. So we will try another model with no seasonal AR coefficient. 

### Model 2 : SARIMA (p,d,q)*(P,D,Q)[12] where p = 2, d= 1, q=1, P=0, D=1, Q=1

```{r}
model_2 <- sarima(tourism.train, 2,1,1,0,1,1,12)
model_2
```

```{r}
model_2$ttable
```

### Model 3 : SARIMA (p,d,q)*(P,D,Q)[12] where p = 2, d= 1, q=1, P=1, D=3, Q=1

```{r}
model_3 <- sarima(tourism.train, 2,1,1,1,3,1,12, no.constant = T)
model_3
```

```{r}
model_3$ttable
```

### Model 4 : SARIMA (p,d,q)*(P,D,Q)[12] where p = 0, d= 0, q=1, P=0, D=2, Q=1

```{r}
model_4 <- sarima(tourism.train, 0,0,1,0,2,1,12, no.constant = T)
model_4
```

```{r}
model_4$ttable
```


#### Matrices to judge a forecast.

The RMSE value: </br>
  
  $RMSE= \sqrt{\frac{\sum(y_{pred}-y_{act})^2}{n}}$ </br>
  
Maximum Absolute Percentage Error (MAPE) : $(\frac{1}{n}\sum|\frac{pred_i-actual_i}{actual_i}|)) \times 100$, 
smaller the better.

As mentioned in the Paper of IJMS, 25/9/2018, Seasonality in Tourism and Forecasting Foreign Tourist Arrival in India, even here we shall measure the Forecast Accuracy using MAPE and RMSE. As mentioned in the paper, "The empirical literature on forecasting suggests using MAPE as it is a good accuracy measure which does not depend on the magnitude of the forecast variable (Mamula, 2015). Lewis (1982) and Baggio & Klobas (2011) suggest a rough scale for the accuracy of a time-series forecasting model based on MAPE – highly accurate (MAPE < 10%), good (10% < MAPE < 20%), reasonable (20% < MAPE < 50%) and inaccurate (MAPE > 50%)" .


A function to show all in one place.

```{r}
prediction_Sarima <- function(original,train, p,d,q,P,D,Q){
  model <- sarima(train, p,d,q,P,D,Q,12, details = FALSE, no.constant = TRUE)
  model_list <- list(model = c(p,d,q,P,D,Q,12), ttable = model$ttable, 
                     AIC = model$AIC, BIC = model$BIC )
  pred <- sarima.for(train, n.ahead = 34 ,p,d,q,P,D,Q,12)
  actual <- window(original, start= c(2017,03), end = c(2020,1))
  rmse <- sqrt(mean(exp(pred$pred)-actual)^2)
  mape <- mean(abs((exp(pred$pred)-actual)/actual))*100
  pred.list <- list(predicted = pred$pred, actual = actual, rmse = rmse , mape = mape)
  return(cat("Model.AIC= ", model_list$AIC, "Model.BIC =", model_list$BIC, "Pred.RMSE= ", pred.list$rmse, "Pred.MAPE=", pred.list$mape, "\n"))

}
```

### Forcasting using various models.

#### Model 1 : SARIMA (2,1,1)X(1,1,1)[12]

We depend our judgement on the following Matrices:

For model : 
1. AIC
2. BIC

For forecasted value: 
1. RMSE
2. MAPE


```{r}
prediction_Sarima(tourism, tourism.train, 2,1,1,1,1,1)
lines(log_tourism)
```

#### Model 2 : SARIMA (2,1,1)X(0,1,1)[12]

```{r}
prediction_Sarima(tourism, tourism.train, 2,1,1,0,1,1)
lines(log_tourism)
```

#### Model 3 : SARIMA (1,0,1)X(0,2,1)[12]

```{r}
prediction_Sarima(tourism, tourism.train, 1,0,1,0,2,1)
lines(log_tourism)
```


#### Model 4 : SARIMA (0,1,2)X(1,2,1)[12]

```{r}
prediction_Sarima(tourism, tourism.train, 0,0,1,0,2,1)
lines(log_tourism)
```

Considering all the above models the best model, seems to be Model 3.  
